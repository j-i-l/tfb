{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98184d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import general_param as gparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87125855",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data = pd.read_pickle(gparams.cleanded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658f414",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Problems\n",
    "\n",
    "1. We do not know the length of the bindig sequence\n",
    "\n",
    "2. We do not know where the binding sequence is situated in the sequence\n",
    "\n",
    "3. The sequences are of various lengths, which is a problem if we want to use them in some form as imput to a ML model\n",
    "\n",
    "4. Most ML models (all in sklearn as far as I know) need numerical imput\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### 4.\n",
    "The sequences need to be converted, options that come to mind:\n",
    "\n",
    "1 One-hot encoding (and either go for a 2d input or concatenate)\n",
    "2 Ordinal encoding\n",
    "3 Split up into 'words' and leverage from text processing tools\n",
    "\n",
    "After some browsing:\n",
    "\n",
    "> In genome analysis, k-mer-based comparison methods have become standard tools.\n",
    "\n",
    "_see: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5406869/_\n",
    "\n",
    "#### Decision\n",
    "\n",
    "**We will convert the sequence into a list of kmers and try a simple bag-of-words approach.**\n",
    "\n",
    "### 3.\n",
    "With a bag-of-words we do not need to adapt the length of the sequences as the output of `sklearn.feature_extraction.text.CountVectorizer` will be used as input for our model(s).\n",
    "\n",
    "### 1. & 2.\n",
    "If we **create k-mer in a rolling-word kind of manner** (i.e. abcdefg > abc, bcd, cde, def, efg) then we can **search for ngrams** and not just single words.\n",
    "If we do so, we simply need to make sure that:\n",
    "\n",
    "1. The k-mer size we choose is smaller than the binding site itself\n",
    "2. The number of ngrams should not be smaller thant the binding site plus twice the k-mer size\n",
    "\n",
    "If we wanted to do this properly, we could get the data from https://pubmed.ncbi.nlm.nih.gov/22887818/ and choose the k-mer size and ngrams size such that [k-mer size, ngrams + 2 * k-mer] covers 95% of the data.\n",
    "\n",
    "#### Decision\n",
    "\n",
    "For simplicity sake, lets **set the kmer size to 5 and ngrams to 30**\n",
    "\n",
    "**Note**: The two parameter were added to `general_params.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8536f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_size = 5\n",
    "ngram_nbr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b3e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmers(sequence: list[str], size: int):\n",
    "    '''\n",
    "    Creat a list of rolling kmer's of size `size` from a sequence\n",
    "    '''\n",
    "    return tuple(''.join(sequence[i: i+size]) for i in range(len(sequence) - size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843d8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data['kmers'] = sequence_data.sequence.apply(lambda x: get_kmers(x, size=kmer_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f731d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.DataFrame({\n",
    "    'kmers': sequence_data.kmers.apply(lambda x: ' '.join(x)),\n",
    "    'labels': sequence_data.strength.apply(lambda x: 1 if x else 0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1509ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.to_pickle(gparams.kmers_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3176a7aa",
   "metadata": {},
   "source": [
    "### Note\n",
    "Applying `CountVectorizer` to the kmers to create the input feature vector of presence/absence of kmers and its ngrams would be part of feature engineering. \n",
    "\n",
    "However, exporting the resulting ndarray to disk would require quite a bit of memory.\n",
    "Therefore, this step will be done in the next part `Model_Selection_and_Tuning.ipynb` directly, so the data can remian in sparse form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c8cbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, ngram_nbr))\n",
    "X = vectorizer.fit_transform(input_data.kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be309ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8871e94a",
   "metadata": {},
   "source": [
    "Now we get the final input data for our model(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "716b2125",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcounters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X\n",
      "File \u001b[0;32m~/.virtualenvs/tfb/lib/python3.10/site-packages/pandas/core/frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/tfb/lib/python3.10/site-packages/pandas/core/frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   3825\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   3838\u001b[0m     ):\n\u001b[1;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/.virtualenvs/tfb/lib/python3.10/site-packages/pandas/core/frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   4534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4535\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.virtualenvs/tfb/lib/python3.10/site-packages/pandas/core/common.py:556\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequire_length_match\u001b[39m(data, index: Index):\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m    Check the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    558\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m         )\n",
      "File \u001b[0;32m~/.virtualenvs/tfb/lib/python3.10/site-packages/scipy/sparse/_base.py:345\u001b[0m, in \u001b[0;36mspmatrix.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix length is ambiguous; use getnnz()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or shape[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "input_data['counters'] = X\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
