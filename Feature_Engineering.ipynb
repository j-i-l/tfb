{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98184d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import general_param as gparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87125855",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data = pd.read_pickle(gparams.cleanded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658f414",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Problems\n",
    "\n",
    "1. We do not know the length of the bindig sequence\n",
    "\n",
    "2. We do not know where the binding sequence is situated in the sequence\n",
    "\n",
    "3. The sequences are of various lengths, which is a problem if we want to use them in some form as imput to a ML model\n",
    "\n",
    "4. Most ML models (all in sklearn as far as I know) need numerical imput\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### 4.\n",
    "The sequences need to be converted, options that come to mind:\n",
    "\n",
    "1 One-hot encoding (and either go for a 2d input or concatenate)\n",
    "2 Ordinal encoding\n",
    "3 Split up into 'words' and leverage from text processing tools\n",
    "\n",
    "After some browsing:\n",
    "\n",
    "> In genome analysis, k-mer-based comparison methods have become standard tools.\n",
    "\n",
    "_see: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5406869/_\n",
    "\n",
    "#### Decision\n",
    "\n",
    "**We will convert the sequence into a list of kmers and try a simple bag-of-words approach.**\n",
    "\n",
    "### 3.\n",
    "With a bag-of-words we do not need to adapt the length of the sequences as the output of `sklearn.feature_extraction.text.CountVectorizer` will be used as input for our model(s).\n",
    "\n",
    "### 1. & 2.\n",
    "If we **create k-mer in a rolling-word kind of manner** (i.e. abcdefg > abc, bcd, cde, def, efg) then we can **search for ngrams** and not just single words.\n",
    "If we do so, we simply need to make sure that:\n",
    "\n",
    "1. The k-mer size we choose is smaller than the binding site itself\n",
    "2. The number of ngrams should not be smaller thant the binding site plus twice the k-mer size\n",
    "\n",
    "If we wanted to do this properly, we could get the data from https://pubmed.ncbi.nlm.nih.gov/22887818/ and choose the k-mer size and ngrams size such that [k-mer size, ngrams + 2 * k-mer] covers 95% of the data.\n",
    "\n",
    "#### Decision\n",
    "\n",
    "For simplicity sake, lets **set the kmer size to 5 and ngrams to 30**\n",
    "\n",
    "**Note**: The two parameter were added to `general_params.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8536f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_size = 5\n",
    "ngram_nbr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b3e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmers(sequence: list[str], size: int):\n",
    "    '''\n",
    "    Create a list of rolling kmer's of size `size` from a sequence\n",
    "    '''\n",
    "    return tuple(''.join(sequence[i: i+size]) for i in range(len(sequence) - size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843d8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data['kmers'] = sequence_data.sequence.apply(lambda x: get_kmers(x, size=kmer_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78297bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.DataFrame({\n",
    "    'kmers': sequence_data.kmers.apply(lambda x: ' '.join(x)),\n",
    "    'labels': sequence_data.strength.apply(lambda x: 1 if x else 0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5f7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.to_pickle(gparams.kmers_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094bd70d",
   "metadata": {},
   "source": [
    "### Note\n",
    "Applying `CountVectorizer` to the kmers to create the input feature vector of presence/absence of kmers and its ngrams would be part of feature engineering. \n",
    "\n",
    "However, exporting the resulting ndarray to disk would require quite a bit of memory.\n",
    "Therefore, this step will be done in the next part `Model_Selection_and_Tuning.ipynb` directly, so the data can remian in sparse form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61fc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, ngram_nbr))\n",
    "# X = vectorizer.fit_transform(input_data.kmers)\n",
    "# input_data['counters'] = X.toarray()\n",
    "# input_data.to_pickle(gparams.kmers_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
