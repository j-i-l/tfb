{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98184d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e96b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_param as gparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87125855",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data = pd.read_pickle(gparams.cleanded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658f414",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Problems\n",
    "\n",
    "1. We do not know the length of the bindig sequence\n",
    "\n",
    "2. We do not know where the binding sequence is situated in the sequence\n",
    "\n",
    "3. The sequences are of various lengths, which is a problem if we want to use them in some form as imput to a ML model\n",
    "\n",
    "4. Most ML models (all in sklearn as far as I know) need numerical imput\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### 4.\n",
    "The sequences need to be converted, options that come to mind:\n",
    "\n",
    "1 One-hot encoding (and either got for a 2d input or concatenate)\n",
    "2 Ordinal encoding\n",
    "3 Split up into 'words' and leverage from text processing tools\n",
    "\n",
    "After some browsing:\n",
    "\n",
    "> In genome analysis, k-mer-based comparison methods have become standard tools.\n",
    "\n",
    "_see: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5406869/_\n",
    "\n",
    "#### Decision\n",
    "\n",
    "**We will convert the sequence into a list of kmers and try a simple bag-of-words approach.**\n",
    "\n",
    "### 3.\n",
    "With a bag-of-words we do not need to adapt the length of the sequences as the output of `sklearn.feature_extraction.text.CountVectorizer` will be used as input for our model(s).\n",
    "\n",
    "### 1. & 2.\n",
    "If we **create k-mer in a rolling-word kind of manner** (i.e. abcdefg > abc, bcd, cde, def, efg) then we can **search for ngrams** and not just single words.\n",
    "If we do so, we simply need to make sure that:\n",
    "\n",
    "1. The k-mer size we choose is smaller than the binding site itself\n",
    "2. The number of ngrams should not be smaller thant the binding site plus twice the k-mer size\n",
    "\n",
    "If we wanted to do this properly, we could get the data from https://pubmed.ncbi.nlm.nih.gov/22887818/ and choose the k-mer size and ngrams size such that [k-mer size, ngrams / k-mer] covers 95% of the data.\n",
    "\n",
    "#### Decision\n",
    "\n",
    "For simplicity sake, lets **set the kmer size to 5 and ngrams to 30**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8536f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_size = 5\n",
    "ngram_nbr = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b3e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmers(sequence: list[str], size: int):\n",
    "    '''\n",
    "    Creat a list of rolling kmer's of size `size` from a sequence\n",
    "    '''\n",
    "    return tuple(''.join(sequence[i: i+size]) for i in range(len(sequence) - size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "843d8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data['kmers'] = sequence_data.sequence.apply(lambda x: get_kmers(x, size=kmer_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4403a1b",
   "metadata": {},
   "source": [
    "Now we get the final input data for our model(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bfdfe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.DataFrame({\n",
    "    'kmers': sequence_data.kmers.apply(lambda x: ' '.join(x)),\n",
    "    'labels': sequence_data.strength.apply(lambda x: 1 if x else 0)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f93962",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "At this point we have our features and the labels and we can move on to model definition, training and hyper parameter tuning.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Herebelow is just a quick preview how how well an support vector classifier performs on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0820281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, ngram_nbr))\n",
    "X = vectorizer.fit_transform(input_data.kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "920c187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, input_data.labels, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10dce9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classify = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a551105d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_classify.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = svc_classify.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a1ae9",
   "metadata": {},
   "source": [
    "Let's get a first glimps at the performance with the https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff386658",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea0fd0",
   "metadata": {},
   "source": [
    "We could use the f1-score for hyperparam tuning https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94787f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y_test, Y_pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
